{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37a5ef89-039d-4ccf-a893-ebbef6097cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wn.VERB\n",
    "\n",
    "def words_lemmatizer(pos_tagged_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tagged_words:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            stem = lemmatizer.lemmatize(word, wn_tag)\n",
    "            lemmatized_words.append(stem)\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "    return lemmatized_words\n",
    "\n",
    "def clean_by_freq(tokenized_words, cut_off_count):\n",
    "    vocab = Counter(tokenized_words)\n",
    "\n",
    "    # 빈도수가 cut_off_count 이하인 단어를 제거하는 코드를 작성해 주세요\n",
    "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
    "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "\n",
    "def clean_by_len(tokenized_words, cut_off_length):\n",
    "    cleaned_words = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        # 길이가 cut_off_length 이하인 단어 제거하는 코드를 작성해 주세요\n",
    "        if len(word) > cut_off_length:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "def clean_by_freq(tokenized_words, cut_off_count):\n",
    "    vocab = Counter(tokenized_words)\n",
    "\n",
    "    # 빈도수가 cut_off_count 이하인 단어를 제거하는 코드를 작성해 주세요\n",
    "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
    "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "\n",
    "def clean_by_len(tokenized_words, cut_off_length):\n",
    "    cleaned_words = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        # 길이가 cut_off_length 이하인 단어 제거하는 코드를 작성해 주세요\n",
    "        if len(word) > cut_off_length:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "def pos_tagger(tokenized_sents):\n",
    "    pos_tagged_words = []\n",
    "\n",
    "    for sentence in tokenized_sents:\n",
    "        # 단어 토큰화\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "    \n",
    "        # 품사 태깅\n",
    "        pos_tagged = pos_tag(tokenized_words)\n",
    "        pos_tagged_words.extend(pos_tagged)\n",
    "    \n",
    "    return pos_tagged_words\n",
    "\n",
    "# 불용어 제거 함수\n",
    "def clean_by_stopwords(tokenized_words, stop_words_set):\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words_set:\n",
    "            cleaned_words.append(word)\n",
    "            \n",
    "    return cleaned_words\n",
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28fdb857-d58c-47eb-bf2f-7fe7db057b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "      <td>[Watching Time Chasers, it obvious that it was...</td>\n",
       "      <td>[(Watching, VBG), (Time, NNP), (Chasers, NNPS)...</td>\n",
       "      <td>[Watching, Time, Chasers, ,, it, obvious, that...</td>\n",
       "      <td>[Watching, Time, Chasers, obvious, that, make,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[I saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(I, PRP), (saw, VBD), (this, DT), (film, NN),...</td>\n",
       "      <td>[I, saw, this, film, about, 20, year, ago, and...</td>\n",
       "      <td>[saw, this, film, about, year, ago, and, remem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n",
       "      <td>[Minor Spoilers In New York, Joan Barnard (Elv...</td>\n",
       "      <td>[(Minor, JJ), (Spoilers, NNS), (In, IN), (New,...</td>\n",
       "      <td>[Minor, Spoilers, In, New, York, ,, Joan, Barn...</td>\n",
       "      <td>[Minor, Spoilers, New, York, Joan, Barnard, El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "      <td>[I went to see this film with a great deal of ...</td>\n",
       "      <td>[(I, PRP), (went, VBD), (to, TO), (see, VB), (...</td>\n",
       "      <td>[I, go, to, see, this, film, with, a, great, d...</td>\n",
       "      <td>[see, this, film, with, great, deal, excitemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "      <td>[Yes, I agree with everyone on this site this ...</td>\n",
       "      <td>[(Yes, UH), (,, ,), (I, PRP), (agree, VBP), (w...</td>\n",
       "      <td>[Yes, ,, I, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[Yes, agree, with, everyone, this, site, this,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jennifer Ehle was sparkling in \\\"Pride and Pre...</td>\n",
       "      <td>[Jennifer Ehle was sparkling in \\\"Pride and Pr...</td>\n",
       "      <td>[(Jennifer, NNP), (Ehle, NNP), (was, VBD), (sp...</td>\n",
       "      <td>[Jennifer, Ehle, be, sparkle, in, \\, '', Pride...</td>\n",
       "      <td>[Jennifer, Ehle, sparkle, Pride, and, Prejudic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n",
       "      <td>[Amy Poehler is a terrific comedian on Saturda...</td>\n",
       "      <td>[(Amy, NNP), (Poehler, NNP), (is, VBZ), (a, DT...</td>\n",
       "      <td>[Amy, Poehler, be, a, terrific, comedian, on, ...</td>\n",
       "      <td>[Amy, Poehler, terrific, comedian, Saturday, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A plane carrying employees of a large biotech ...</td>\n",
       "      <td>[A plane carrying employees of a large biotech...</td>\n",
       "      <td>[(A, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "      <td>[A, plane, carry, employee, of, a, large, biot...</td>\n",
       "      <td>[plane, carry, employee, large, biotech, firm,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[A well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(A, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "      <td>[A, well, make, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[well, make, gritty, science, fiction, movie, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[Incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(Incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "      <td>[Incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[Incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Watching Time Chasers, it obvious that it was ...   \n",
       "1  I saw this film about 20 years ago and remembe...   \n",
       "2  Minor Spoilers In New York, Joan Barnard (Elvi...   \n",
       "3  I went to see this film with a great deal of e...   \n",
       "4  Yes, I agree with everyone on this site this m...   \n",
       "5  Jennifer Ehle was sparkling in \\\"Pride and Pre...   \n",
       "6  Amy Poehler is a terrific comedian on Saturday...   \n",
       "7  A plane carrying employees of a large biotech ...   \n",
       "8  A well made, gritty science fiction movie, it ...   \n",
       "9  Incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [Watching Time Chasers, it obvious that it was...   \n",
       "1  [I saw this film about 20 years ago and rememb...   \n",
       "2  [Minor Spoilers In New York, Joan Barnard (Elv...   \n",
       "3  [I went to see this film with a great deal of ...   \n",
       "4  [Yes, I agree with everyone on this site this ...   \n",
       "5  [Jennifer Ehle was sparkling in \\\"Pride and Pr...   \n",
       "6  [Amy Poehler is a terrific comedian on Saturda...   \n",
       "7  [A plane carrying employees of a large biotech...   \n",
       "8  [A well made, gritty science fiction movie, it...   \n",
       "9  [Incredibly dumb and utterly predictable story...   \n",
       "\n",
       "                                   pos_tagged_tokens  \\\n",
       "0  [(Watching, VBG), (Time, NNP), (Chasers, NNPS)...   \n",
       "1  [(I, PRP), (saw, VBD), (this, DT), (film, NN),...   \n",
       "2  [(Minor, JJ), (Spoilers, NNS), (In, IN), (New,...   \n",
       "3  [(I, PRP), (went, VBD), (to, TO), (see, VB), (...   \n",
       "4  [(Yes, UH), (,, ,), (I, PRP), (agree, VBP), (w...   \n",
       "5  [(Jennifer, NNP), (Ehle, NNP), (was, VBD), (sp...   \n",
       "6  [(Amy, NNP), (Poehler, NNP), (is, VBZ), (a, DT...   \n",
       "7  [(A, DT), (plane, NN), (carrying, VBG), (emplo...   \n",
       "8  [(A, DT), (well, NN), (made, VBN), (,, ,), (gr...   \n",
       "9  [(Incredibly, RB), (dumb, JJ), (and, CC), (utt...   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [Watching, Time, Chasers, ,, it, obvious, that...   \n",
       "1  [I, saw, this, film, about, 20, year, ago, and...   \n",
       "2  [Minor, Spoilers, In, New, York, ,, Joan, Barn...   \n",
       "3  [I, go, to, see, this, film, with, a, great, d...   \n",
       "4  [Yes, ,, I, agree, with, everyone, on, this, s...   \n",
       "5  [Jennifer, Ehle, be, sparkle, in, \\, '', Pride...   \n",
       "6  [Amy, Poehler, be, a, terrific, comedian, on, ...   \n",
       "7  [A, plane, carry, employee, of, a, large, biot...   \n",
       "8  [A, well, make, ,, gritty, science, fiction, m...   \n",
       "9  [Incredibly, dumb, and, utterly, predictable, ...   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  [Watching, Time, Chasers, obvious, that, make,...  \n",
       "1  [saw, this, film, about, year, ago, and, remem...  \n",
       "2  [Minor, Spoilers, New, York, Joan, Barnard, El...  \n",
       "3  [see, this, film, with, great, deal, excitemen...  \n",
       "4  [Yes, agree, with, everyone, this, site, this,...  \n",
       "5  [Jennifer, Ehle, sparkle, Pride, and, Prejudic...  \n",
       "6  [Amy, Poehler, terrific, comedian, Saturday, N...  \n",
       "7  [plane, carry, employee, large, biotech, firm,...  \n",
       "8  [well, make, gritty, science, fiction, movie, ...  \n",
       "9  [Incredibly, dumb, and, utterly, predictable, ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "df = pd.read_csv(\"imdb.tsv\", sep = '\\t')\n",
    "del df['Unnamed: 0']\n",
    "df['review'].str.lower() #소문자 바꾸기\n",
    "df[\"sent_tokens\"] = df['review'].apply(sent_tokenize) #문장단위로 토큰화\n",
    "df[\"pos_tagged_tokens\"] = df[\"sent_tokens\"].apply(pos_tagger) #품사 쪼개기 = 형태소\n",
    "df[\"lemmatized_tokens\"] = df[\"pos_tagged_tokens\"].apply(words_lemmatizer) #원형으로 바꾸기\n",
    "\n",
    "#두글자 이하는 삭제\n",
    "df[\"cleaned_tokens\"] = df['lemmatized_tokens'].apply(lambda x : clean_by_len(x,2))\n",
    "# 불용어 처리\n",
    "df['cleaned_tokens'].apply(lambda x : clean_by_stopwords(x, stopwords_set))\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
